<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>CS270 Midterm — Neural Network Rules</title>
<link href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&family=Syne:wght@400;700;800&display=swap" rel="stylesheet">
<style>
  :root {
    --bg: #0a0a0f;
    --surface: #12121a;
    --card: #1a1a26;
    --border: #2a2a40;
    --accent: #00e5ff;
    --accent2: #ff4d6d;
    --accent3: #a8ff78;
    --text: #e8e8f0;
    --muted: #6b6b8a;
    --correct: #a8ff78;
    --wrong: #ff4d6d;
    --warn: #ffd166;
  }
  * { box-sizing: border-box; margin: 0; padding: 0; }
  body {
    background: var(--bg);
    color: var(--text);
    font-family: 'Space Mono', monospace;
    min-height: 100vh;
    overflow-x: hidden;
  }

  /* Grid noise background */
  body::before {
    content: '';
    position: fixed; inset: 0;
    background-image: 
      linear-gradient(var(--border) 1px, transparent 1px),
      linear-gradient(90deg, var(--border) 1px, transparent 1px);
    background-size: 40px 40px;
    opacity: 0.18;
    pointer-events: none;
    z-index: 0;
  }

  .container { position: relative; z-index: 1; max-width: 960px; margin: 0 auto; padding: 32px 20px 80px; }

  header {
    border-left: 4px solid var(--accent);
    padding-left: 20px;
    margin-bottom: 48px;
  }
  header .label { color: var(--accent); font-size: 11px; letter-spacing: 4px; text-transform: uppercase; margin-bottom: 8px; }
  header h1 { font-family: 'Syne', sans-serif; font-size: clamp(28px, 5vw, 48px); font-weight: 800; line-height: 1.1; }
  header h1 span { color: var(--accent2); }
  header p { color: var(--muted); font-size: 13px; margin-top: 10px; }

  /* Tab nav */
  .tabs { display: flex; gap: 4px; margin-bottom: 32px; flex-wrap: wrap; }
  .tab {
    background: var(--surface); border: 1px solid var(--border);
    color: var(--muted); font-family: 'Space Mono', monospace;
    font-size: 11px; letter-spacing: 2px; text-transform: uppercase;
    padding: 10px 18px; cursor: pointer; transition: all .2s;
  }
  .tab:hover { color: var(--text); border-color: var(--accent); }
  .tab.active { background: var(--accent); color: var(--bg); border-color: var(--accent); font-weight: 700; }

  .panel { display: none; }
  .panel.active { display: block; animation: fadeIn .3s ease; }
  @keyframes fadeIn { from { opacity: 0; transform: translateY(8px); } to { opacity: 1; transform: translateY(0); } }

  /* Cards */
  .card {
    background: var(--card);
    border: 1px solid var(--border);
    padding: 24px 28px;
    margin-bottom: 20px;
  }
  .card-title {
    font-family: 'Syne', sans-serif; font-size: 13px;
    letter-spacing: 3px; text-transform: uppercase;
    color: var(--accent); margin-bottom: 16px;
  }
  .card p { font-size: 13px; line-height: 1.9; color: var(--text); }
  .card p + p { margin-top: 12px; }

  /* Formula blocks */
  .formula {
    background: var(--bg);
    border-left: 3px solid var(--accent2);
    padding: 14px 18px;
    margin: 16px 0;
    font-size: 14px;
    color: var(--warn);
    letter-spacing: 0.5px;
  }
  .formula .comment { color: var(--muted); font-size: 11px; }

  /* Comparison grid */
  .compare-grid {
    display: grid; grid-template-columns: 1fr 1fr; gap: 16px;
    margin-top: 16px;
  }
  .compare-col { background: var(--bg); border: 1px solid var(--border); padding: 18px; }
  .compare-col h3 { font-family: 'Syne', sans-serif; font-size: 13px; margin-bottom: 12px; }
  .compare-col.delta h3 { color: var(--accent); }
  .compare-col.percep h3 { color: var(--accent2); }
  .compare-col ul { list-style: none; }
  .compare-col ul li { font-size: 12px; line-height: 1.8; color: var(--text); padding-left: 14px; position: relative; }
  .compare-col ul li::before { content: '▸'; position: absolute; left: 0; }
  .compare-col.delta ul li::before { color: var(--accent); }
  .compare-col.percep ul li::before { color: var(--accent2); }

  /* Sigmoid canvas */
  #sigmoidCanvas { 
    display: block; background: var(--bg); border: 1px solid var(--border);
    margin: 16px auto; cursor: crosshair;
  }
  .slider-row { display: flex; align-items: center; gap: 16px; margin: 12px 0; flex-wrap: wrap; }
  .slider-row label { font-size: 12px; color: var(--muted); min-width: 100px; }
  .slider-row input[type=range] { flex: 1; min-width: 120px; accent-color: var(--accent); }
  .slider-val { color: var(--accent); font-size: 13px; min-width: 50px; }

  /* Line equation viz */
  #lineCanvas { display: block; background: var(--bg); border: 1px solid var(--border); margin: 16px auto; cursor: crosshair; }

  /* Relationship map */
  .rel-map {
    display: grid;
    grid-template-areas:
      ". input ."
      "weights net output"
      ". activation ."
      ". error ."
      ". update .";
    gap: 10px;
    margin: 16px 0;
  }
  .rel-node {
    background: var(--bg); border: 1px solid var(--border);
    padding: 12px 10px; text-align: center; font-size: 11px;
    line-height: 1.5; transition: all .3s; cursor: pointer;
    position: relative;
  }
  .rel-node:hover { border-color: var(--accent); color: var(--accent); }
  .rel-node .node-label { font-family: 'Syne', sans-serif; font-weight: 700; font-size: 12px; }
  .rel-node .node-formula { color: var(--warn); font-size: 10px; margin-top: 4px; }
  .rel-node[data-area] { grid-area: attr(data-area); }
  .n-input { grid-area: input; border-color: var(--accent3); }
  .n-weights { grid-area: weights; border-color: var(--accent); }
  .n-net { grid-area: net; border-color: var(--warn); }
  .n-output { grid-area: output; border-color: var(--accent2); }
  .n-activation { grid-area: activation; border-color: var(--accent2); }
  .n-error { grid-area: error; border-color: var(--warn); }
  .n-update { grid-area: update; border-color: var(--accent3); }
  .node-desc {
    background: var(--surface); border: 1px solid var(--border);
    padding: 14px 18px; font-size: 12px; line-height: 1.8;
    margin-top: 12px; min-height: 60px;
    transition: all .3s;
  }

  /* Quiz */
  .quiz-question {
    background: var(--card); border: 1px solid var(--border);
    padding: 24px 28px; margin-bottom: 20px;
  }
  .q-text { font-size: 14px; line-height: 1.8; margin-bottom: 20px; color: var(--text); }
  .q-text code { background: var(--bg); padding: 2px 6px; color: var(--warn); font-family: 'Space Mono'; font-size: 13px; }
  .options { display: flex; flex-direction: column; gap: 8px; }
  .opt {
    background: var(--bg); border: 1px solid var(--border);
    padding: 12px 16px; cursor: pointer; font-size: 12px;
    font-family: 'Space Mono'; text-align: left; color: var(--text);
    transition: all .2s; line-height: 1.6;
  }
  .opt:hover:not(.revealed) { border-color: var(--accent); color: var(--accent); }
  .opt.selected { border-color: var(--accent); background: rgba(0,229,255,0.05); }
  .opt.correct { border-color: var(--correct) !important; background: rgba(168,255,120,0.08) !important; color: var(--correct) !important; }
  .opt.wrong { border-color: var(--wrong) !important; background: rgba(255,77,109,0.08) !important; color: var(--wrong) !important; }
  .explanation {
    margin-top: 16px; padding: 14px 18px;
    background: var(--bg); border-left: 3px solid var(--accent3);
    font-size: 12px; line-height: 1.9; display: none; color: var(--text);
  }
  .explanation.show { display: block; }
  .explanation strong { color: var(--accent3); }

  .score-bar {
    display: flex; align-items: center; gap: 20px;
    padding: 16px 20px; background: var(--surface); border: 1px solid var(--border);
    margin-bottom: 24px; font-size: 13px;
  }
  .score-bar .score-num { font-family: 'Syne', sans-serif; font-size: 24px; font-weight: 800; color: var(--accent); }
  .score-bar .score-label { color: var(--muted); font-size: 11px; letter-spacing: 2px; text-transform: uppercase; }

  .btn {
    background: transparent; border: 1px solid var(--accent);
    color: var(--accent); font-family: 'Space Mono'; font-size: 12px;
    letter-spacing: 2px; text-transform: uppercase; padding: 10px 24px;
    cursor: pointer; transition: all .2s; margin-top: 8px;
  }
  .btn:hover { background: var(--accent); color: var(--bg); }
  .btn.danger { border-color: var(--accent2); color: var(--accent2); }
  .btn.danger:hover { background: var(--accent2); color: var(--bg); }

  table { width: 100%; border-collapse: collapse; margin: 16px 0; font-size: 12px; }
  th { background: var(--bg); color: var(--accent); padding: 10px 14px; text-align: left; border: 1px solid var(--border); letter-spacing: 1px; }
  td { padding: 10px 14px; border: 1px solid var(--border); color: var(--text); line-height: 1.6; }
  tr:hover td { background: rgba(255,255,255,0.02); }

  .highlight { color: var(--accent); font-weight: 700; }
  .highlight2 { color: var(--accent2); font-weight: 700; }
  .highlight3 { color: var(--accent3); font-weight: 700; }
  .warn { color: var(--warn); }

  .divider { border: none; border-top: 1px solid var(--border); margin: 24px 0; }

  @media(max-width: 600px) {
    .compare-grid { grid-template-columns: 1fr; }
    .rel-map { grid-template-areas: "input" "weights" "net" "output" "activation" "error" "update"; grid-template-columns: 1fr; }
  }
</style>
</head>
<body>
<div class="container">
  <header>
    <div class="label">CS270 · Midterm Prep</div>
    <h1>Neural Networks:<br><span>Delta vs Perceptron</span></h1>
    <p>Interactive study guide — sigmoid intuition, line equations, relationship mapping, hard MCQs</p>
  </header>

  <div class="tabs">
    <button class="tab active" onclick="switchTab('concepts')">Concepts</button>
    <button class="tab" onclick="switchTab('relationships')">Relationships Map</button>
    <button class="tab" onclick="switchTab('math')">Math Viz</button>
    <button class="tab" onclick="switchTab('quiz')">MCQ Quiz</button>
  </div>

  <!-- CONCEPTS -->
  <div id="tab-concepts" class="panel active">

    <div class="card">
      <div class="card-title">The Core Setup</div>
      <p>Both learning rules adjust <span class="highlight">weights</span> based on the difference between the <span class="highlight2">target output (t)</span> and the <span class="highlight3">actual output (y)</span>. The key difference is <em>what counts as the output</em> and <em>when</em> the adjustment is applied.</p>
      <div class="formula">
        net input: &nbsp; <strong>net = Σ wᵢ · xᵢ</strong> &nbsp; (sum of weight × input, including bias)
        <div class="comment"># The "raw" score before any activation function</div>
      </div>
    </div>

    <div class="card">
      <div class="card-title">Perceptron Learning Rule</div>
      <p>Output is <span class="highlight2">binary (0 or 1)</span> — a hard threshold is applied FIRST, then weights update based on the thresholded output.</p>
      <div class="formula">
        y = <strong>threshold(net)</strong> &nbsp; → &nbsp; 1 if net ≥ θ, else 0
        <br><br>
        Δwᵢ = <strong>η · (t − y) · xᵢ</strong>
        <div class="comment"># t and y are BOTH binary (0 or 1). Error is -1, 0, or +1</div>
      </div>
      <p>Only updates when there's a <strong>misclassification</strong>. If y == t, Δw = 0. The rule <span class="highlight">converges for linearly separable data</span> (Perceptron Convergence Theorem).</p>
    </div>

    <div class="card">
      <div class="card-title">Delta Rule (Widrow-Hoff / LMS)</div>
      <p>Output is the <span class="highlight">raw net input</span> (linear, no threshold). The error is computed in continuous space, so gradients are well-defined.</p>
      <div class="formula">
        y = <strong>net</strong> &nbsp; (no activation function in the weight update)
        <br><br>
        Δwᵢ = <strong>η · (t − net) · xᵢ</strong>
        <div class="comment"># t is continuous target; error can be any real number</div>
      </div>
      <p>This is <span class="highlight3">gradient descent on the sum-squared error</span>: E = ½ Σ (t − net)². It <span class="highlight">always converges</span> to a minimum even for non-linearly-separable data (will find best linear approximation).</p>
    </div>

    <div class="compare-grid">
      <div class="compare-col delta">
        <h3>DELTA RULE</h3>
        <ul>
          <li>y = net (linear output)</li>
          <li>Error: t − net (continuous)</li>
          <li>Minimizes MSE</li>
          <li>Always converges</li>
          <li>Works on non-sep. data</li>
          <li>Gradient descent basis</li>
          <li>Foundation for backprop</li>
        </ul>
      </div>
      <div class="compare-col percep">
        <h3>PERCEPTRON RULE</h3>
        <ul>
          <li>y = threshold(net) (binary)</li>
          <li>Error: t − y ∈ {-1,0,1}</li>
          <li>Minimizes misclassifications</li>
          <li>Converges only if lin. sep.</li>
          <li>Fails on XOR etc.</li>
          <li>NOT gradient descent</li>
          <li>Decision boundary finder</li>
        </ul>
      </div>
    </div>

    <div class="card">
      <div class="card-title">Critical Distinctions (Exam Traps)</div>
      <table>
        <tr><th>Property</th><th>Perceptron</th><th>Delta Rule</th></tr>
        <tr><td>Output used in Δw</td><td>Thresholded binary y</td><td>Raw net (no threshold)</td></tr>
        <tr><td>Error magnitude</td><td>Only −1, 0, +1</td><td>Any real number</td></tr>
        <tr><td>Updates when correct?</td><td>No (error = 0)</td><td>Yes, if net ≠ t</td></tr>
        <tr><td>Convergence guarantee</td><td>Lin. sep. only</td><td>Always (to min MSE)</td></tr>
        <tr><td>Type of optimization</td><td>NOT gradient descent</td><td>IS gradient descent</td></tr>
        <tr><td>Activation at update time</td><td>After threshold</td><td>Before threshold</td></tr>
      </table>
      <p class="warn" style="margin-top:12px;">⚠ Trap: Both use η(t−something)xᵢ. The difference is whether "something" is the raw net or the thresholded output.</p>
    </div>
  </div>

  <!-- RELATIONSHIPS MAP -->
  <div id="tab-relationships" class="panel">
    <div class="card">
      <div class="card-title">Click a node to explore its role</div>
      <p>This maps the data flow in a single-layer network and shows where each rule diverges.</p>
    </div>

    <div class="rel-map">
      <div class="rel-node n-input" onclick="showDesc('input')">
        <div class="node-label">INPUTS</div>
        <div class="node-formula">x₀=1, x₁, x₂…xₙ</div>
      </div>
      <div class="rel-node n-weights" onclick="showDesc('weights')">
        <div class="node-label">WEIGHTS</div>
        <div class="node-formula">w₀(bias), w₁…wₙ</div>
      </div>
      <div class="rel-node n-net" onclick="showDesc('net')">
        <div class="node-label">NET INPUT</div>
        <div class="node-formula">net = Σ wᵢxᵢ</div>
      </div>
      <div class="rel-node n-output" onclick="showDesc('output')">
        <div class="node-label">OUTPUT y</div>
        <div class="node-formula">⚡ FORK POINT</div>
      </div>
      <div class="rel-node n-activation" onclick="showDesc('activation')">
        <div class="node-label">ACTIVATION</div>
        <div class="node-formula">threshold / sigmoid</div>
      </div>
      <div class="rel-node n-error" onclick="showDesc('error')">
        <div class="node-label">ERROR</div>
        <div class="node-formula">t − y</div>
      </div>
      <div class="rel-node n-update" onclick="showDesc('update')">
        <div class="node-label">WEIGHT UPDATE</div>
        <div class="node-formula">Δwᵢ = η(t−y)xᵢ</div>
      </div>
    </div>

    <div class="node-desc" id="node-desc">← Click a node above to see details</div>

    <div class="card" style="margin-top:20px;">
      <div class="card-title">The Fork: Where Rules Diverge</div>
      <p>After computing <span class="highlight">net</span>, the two rules branch:</p>
      <div class="formula">
        <span style="color:var(--accent)">DELTA:</span> &nbsp; use y = net directly → error = t − net → Δwᵢ = η(t−net)xᵢ
        <br><br>
        <span style="color:var(--accent2)">PERCEPTRON:</span> &nbsp; apply threshold → y ∈ {0,1} → error = t − y → Δwᵢ = η(t−y)xᵢ
      </div>
      <p>Both then loop back to update weights. The cycle repeats until convergence (or max iterations).</p>
    </div>
  </div>

  <!-- MATH VIZ -->
  <div id="tab-math" class="panel">
    <div class="card">
      <div class="card-title">Line Equations as Decision Boundaries</div>
      <p>A single neuron with 2 inputs computes: <span class="formula" style="display:inline; border:none; padding:2px 6px;">w₀ + w₁x₁ + w₂x₂ = 0</span></p>
      <p>This is a <strong>line</strong> in 2D space. Points where the net is positive → one class; negative → other. Drag the sliders to see the boundary move.</p>
      <canvas id="lineCanvas" width="580" height="320"></canvas>
      <div class="slider-row">
        <label>w₀ (bias):</label>
        <input type="range" id="bias" min="-4" max="4" step="0.1" value="-0.5" oninput="drawLine()">
        <span class="slider-val" id="biasVal">-0.5</span>
      </div>
      <div class="slider-row">
        <label>w₁ (x-weight):</label>
        <input type="range" id="w1" min="-4" max="4" step="0.1" value="1.5" oninput="drawLine()">
        <span class="slider-val" id="w1Val">1.5</span>
      </div>
      <div class="slider-row">
        <label>w₂ (y-weight):</label>
        <input type="range" id="w2" min="-4" max="4" step="0.1" value="1.0" oninput="drawLine()">
        <span class="slider-val" id="w2Val">1.0</span>
      </div>
      <p class="warn" style="font-size:12px; margin-top:8px;">Line eq: w₀ + w₁·x₁ + w₂·x₂ = 0 → x₂ = −(w₁/w₂)·x₁ − (w₀/w₂) &nbsp; (slope-intercept form)</p>
    </div>

    <div class="card">
      <div class="card-title">Sigmoid Function — Intuition</div>
      <p>σ(x) = 1 / (1 + e<sup>−x</sup>). Squashes any real number to (0,1). Used in logistic neurons and as the smooth differentiable analog of threshold activation.</p>
      <canvas id="sigmoidCanvas" width="580" height="280"></canvas>
      <div class="slider-row">
        <label>Steepness (k):</label>
        <input type="range" id="sigK" min="0.1" max="10" step="0.1" value="1" oninput="drawSigmoid()">
        <span class="slider-val" id="kVal">1.0</span>
      </div>
      <div class="slider-row">
        <label>Shift (x₀):</label>
        <input type="range" id="sigShift" min="-4" max="4" step="0.1" value="0" oninput="drawSigmoid()">
        <span class="slider-val" id="shiftVal">0.0</span>
      </div>
      <p>σ(x) = 1 / (1 + e<sup>−k(x−x₀)</sup></p>
      <p class="warn" style="font-size:12px; margin-top:8px;">⚠ As k→∞, sigmoid → threshold function. This is the link between Delta rule and Perceptron.</p>
      <p style="font-size:12px; margin-top:8px;">Key property: <span class="highlight">σ'(x) = σ(x)·(1−σ(x))</span> — the derivative is computable from the output itself. This makes backpropagation efficient.</p>
    </div>
  </div>

  <!-- QUIZ -->
  <div id="tab-quiz" class="panel">
    <div class="score-bar">
      <div>
        <div class="score-num" id="scoreNum">0/0</div>
        <div class="score-label">Score</div>
      </div>
      <button class="btn danger" onclick="resetQuiz()">Reset Quiz</button>
    </div>

    <div id="quiz-container"></div>
  </div>
</div>

<script>
// Tab switching
function switchTab(name) {
  document.querySelectorAll('.panel').forEach(p => p.classList.remove('active'));
  document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
  document.getElementById('tab-' + name).classList.add('active');
  event.target.classList.add('active');
  if (name === 'math') { drawLine(); drawSigmoid(); }
}

// Node descriptions
const nodeDescs = {
  input: `<strong style="color:var(--accent3)">INPUTS (xᵢ)</strong><br>
    Raw feature values fed into the network. x₀ is always 1 (the bias input). 
    Inputs are fixed — they don't change during learning. The bias term (x₀=1, w₀) lets the 
    decision boundary shift away from the origin, equivalent to the intercept in a line equation.`,
  weights: `<strong style="color:var(--accent)">WEIGHTS (wᵢ)</strong><br>
    The learnable parameters. w₀ is the bias weight. Each weight scales how much its corresponding 
    input contributes. The weight vector defines the orientation and position of the decision hyperplane. 
    Both rules modify these via Δwᵢ = η·(t−y)·xᵢ — but y means different things in each.`,
  net: `<strong style="color:var(--warn)">NET INPUT</strong><br>
    net = Σ wᵢxᵢ = w₀ + w₁x₁ + w₂x₂ + …<br>
    This is the raw dot product of weights and inputs. It represents which side of the decision 
    boundary the input falls on. <span style="color:var(--accent)">Delta rule uses this directly as the output y.</span>
    <span style="color:var(--accent2)">Perceptron passes it through threshold first.</span>`,
  output: `<strong style="color:var(--accent2)">OUTPUT — THE FORK POINT ⚡</strong><br>
    This is where the two rules diverge completely.<br>
    <span style="color:var(--accent)">Delta Rule:</span> y = net (no nonlinearity). Output is continuous.<br>
    <span style="color:var(--accent2)">Perceptron:</span> y = threshold(net) → 0 or 1. Output is binary.<br>
    This single difference changes everything: convergence properties, error magnitude, 
    what the rule is optimizing, and whether it generalizes to multilayer networks.`,
  activation: `<strong style="color:var(--accent2)">ACTIVATION FUNCTION</strong><br>
    Applied to net to produce the final output. Options include:<br>
    • Hard threshold: out = 1 if net≥θ else 0 (Perceptron)<br>
    • Linear: out = net (Delta Rule)<br>
    • Sigmoid: out = 1/(1+e⁻ⁿᵉᵗ) (logistic neuron / backprop)<br>
    • ReLU, tanh, etc. (modern networks)<br>
    The choice of activation determines differentiability and what kind of gradient descent is possible.`,
  error: `<strong style="color:var(--warn)">ERROR SIGNAL (t − y)</strong><br>
    The difference between target t and actual output y.<br>
    <span style="color:var(--accent)">Delta Rule error:</span> t − net → continuous, can be large<br>
    <span style="color:var(--accent2)">Perceptron error:</span> t − threshold(net) → only −1, 0, or +1<br>
    The Delta rule error is the gradient of ½(t−net)². The Perceptron error is NOT a gradient of any loss function — it's just a heuristic correction signal.`,
  update: `<strong style="color:var(--accent3)">WEIGHT UPDATE</strong><br>
    Δwᵢ = η · (t−y) · xᵢ where η is the learning rate.<br>
    This formula looks identical for both rules, but y differs.<br>
    The update pulls the decision boundary toward correctly classifying the current example. 
    η controls step size: too large → oscillation; too small → slow convergence. 
    After update, weights feed back into the next forward pass.`
};

function showDesc(key) {
  document.getElementById('node-desc').innerHTML = nodeDescs[key];
}

// Line Viz
function drawLine() {
  const canvas = document.getElementById('lineCanvas');
  if (!canvas) return;
  const ctx = canvas.getContext('2d');
  const W = canvas.width, H = canvas.height;
  const b = parseFloat(document.getElementById('bias').value);
  const w1 = parseFloat(document.getElementById('w1').value);
  const w2 = parseFloat(document.getElementById('w2').value);
  document.getElementById('biasVal').textContent = b.toFixed(1);
  document.getElementById('w1Val').textContent = w1.toFixed(1);
  document.getElementById('w2Val').textContent = w2.toFixed(1);

  ctx.clearRect(0, 0, W, H);
  ctx.fillStyle = '#0a0a0f';
  ctx.fillRect(0, 0, W, H);

  // Grid
  ctx.strokeStyle = '#1a1a26';
  ctx.lineWidth = 1;
  const cx = W/2, cy = H/2, scale = 50;
  for (let i = -6; i <= 6; i++) {
    ctx.beginPath(); ctx.moveTo(cx + i*scale, 0); ctx.lineTo(cx + i*scale, H); ctx.stroke();
    ctx.beginPath(); ctx.moveTo(0, cy + i*scale); ctx.lineTo(W, cy + i*scale); ctx.stroke();
  }
  // Axes
  ctx.strokeStyle = '#2a2a40'; ctx.lineWidth = 2;
  ctx.beginPath(); ctx.moveTo(0, cy); ctx.lineTo(W, cy); ctx.stroke();
  ctx.beginPath(); ctx.moveTo(cx, 0); ctx.lineTo(cx, H); ctx.stroke();

  // Color regions
  ctx.save();
  if (Math.abs(w2) > 0.01) {
    const grad = ctx.createLinearGradient(0, 0, 0, H);
    grad.addColorStop(0, 'rgba(0,229,255,0.07)');
    grad.addColorStop(1, 'rgba(255,77,109,0.07)');
    ctx.fillStyle = grad;
    ctx.fillRect(0, 0, W, H);
  }
  ctx.restore();

  // Decision boundary line: w0 + w1*x1 + w2*x2 = 0 → x2 = -(w1*x1 + w0)/w2
  ctx.strokeStyle = '#00e5ff'; ctx.lineWidth = 2.5;
  ctx.beginPath();
  if (Math.abs(w2) > 0.01) {
    for (let px = 0; px <= W; px++) {
      const x1 = (px - cx) / scale;
      const x2 = -(w1 * x1 + b) / w2;
      const py = cy - x2 * scale;
      if (px === 0) ctx.moveTo(px, py); else ctx.lineTo(px, py);
    }
  } else {
    // Vertical line: w0 + w1*x1 = 0 → x1 = -w0/w1
    if (Math.abs(w1) > 0.01) {
      const x1 = -b / w1;
      const px = cx + x1 * scale;
      ctx.moveTo(px, 0); ctx.lineTo(px, H);
    }
  }
  ctx.stroke();

  // Labels
  ctx.fillStyle = '#00e5ff'; ctx.font = '11px Space Mono';
  ctx.fillText('w₀=' + b.toFixed(1) + '  w₁=' + w1.toFixed(1) + '  w₂=' + w2.toFixed(1), 12, 20);
  ctx.fillStyle = 'rgba(0,229,255,0.6)'; ctx.fillText('net > 0', 20, 50);
  ctx.fillStyle = 'rgba(255,77,109,0.6)'; ctx.fillText('net < 0', 20, H-20);
  ctx.fillStyle = '#6b6b8a'; ctx.fillText('x₁', W-20, cy-8); ctx.fillText('x₂', cx+8, 14);
}

// Sigmoid Viz
function drawSigmoid() {
  const canvas = document.getElementById('sigmoidCanvas');
  if (!canvas) return;
  const ctx = canvas.getContext('2d');
  const W = canvas.width, H = canvas.height;
  const k = parseFloat(document.getElementById('sigK').value);
  const shift = parseFloat(document.getElementById('sigShift').value);
  document.getElementById('kVal').textContent = k.toFixed(1);
  document.getElementById('shiftVal').textContent = shift.toFixed(1);

  ctx.clearRect(0, 0, W, H);
  ctx.fillStyle = '#0a0a0f'; ctx.fillRect(0, 0, W, H);

  const cx = W/2, cy = H/2, scaleX = 50, scaleY = 100;

  // Grid
  ctx.strokeStyle = '#1a1a26'; ctx.lineWidth = 1;
  for (let i = -6; i <= 6; i++) {
    ctx.beginPath(); ctx.moveTo(cx + i*scaleX, 0); ctx.lineTo(cx + i*scaleX, H); ctx.stroke();
  }
  [0, 0.5, 1].forEach(v => {
    const py = cy - (v - 0.5) * scaleY * 2;
    ctx.beginPath(); ctx.moveTo(0, py); ctx.lineTo(W, py); ctx.stroke();
    ctx.fillStyle = '#2a2a40'; ctx.font = '10px Space Mono';
    ctx.fillText(v.toFixed(1), 4, py - 3);
  });

  ctx.strokeStyle = '#2a2a40'; ctx.lineWidth = 2;
  ctx.beginPath(); ctx.moveTo(0, cy); ctx.lineTo(W, cy); ctx.stroke();
  ctx.beginPath(); ctx.moveTo(cx, 0); ctx.lineTo(cx, H); ctx.stroke();

  // Sigmoid curve
  const sigmoid = x => 1 / (1 + Math.exp(-k * (x - shift)));
  ctx.strokeStyle = '#00e5ff'; ctx.lineWidth = 2.5; ctx.beginPath();
  for (let px = 0; px <= W; px++) {
    const x = (px - cx) / scaleX;
    const y = sigmoid(x);
    const py = cy - (y - 0.5) * scaleY * 2;
    px === 0 ? ctx.moveTo(px, py) : ctx.lineTo(px, py);
  }
  ctx.stroke();

  // Derivative curve (in orange)
  ctx.strokeStyle = 'rgba(255,209,102,0.6)'; ctx.lineWidth = 1.5; ctx.setLineDash([4,4]);
  ctx.beginPath();
  for (let px = 0; px <= W; px++) {
    const x = (px - cx) / scaleX;
    const s = sigmoid(x);
    const deriv = k * s * (1 - s);
    const py = cy - (deriv - 0.5) * scaleY * 2;
    px === 0 ? ctx.moveTo(px, py) : ctx.lineTo(px, py);
  }
  ctx.stroke();
  ctx.setLineDash([]);

  // Point at x=0
  const y0 = sigmoid(0);
  const py0 = cy - (y0 - 0.5) * scaleY * 2;
  ctx.fillStyle = '#ff4d6d'; ctx.beginPath(); ctx.arc(cx + shift*scaleX, py0, 5, 0, Math.PI*2); ctx.fill();

  ctx.fillStyle = '#00e5ff'; ctx.font = '11px Space Mono';
  ctx.fillText('σ(x) = 1/(1+e^(-k(x-x₀)))   k=' + k.toFixed(1) + '  x₀=' + shift.toFixed(1), 12, 18);
  ctx.fillStyle = 'rgba(255,209,102,0.8)';
  ctx.fillText("- - - σ'(x) = k·σ(x)·(1-σ(x))", 12, 36);
}

// Quiz Data
const questions = [
  {
    q: "In the Perceptron learning rule, what is the output <code>y</code> used to compute the weight update Δwᵢ = η(t−y)xᵢ?",
    opts: [
      "The raw net input: y = Σ wᵢxᵢ",
      "The thresholded binary value: y = 1 if net ≥ θ, else 0",
      "The sigmoid activation: y = 1/(1+e^{-net})",
      "The gradient of the error: y = d/dw(½(t−net)²)"
    ],
    ans: 1,
    exp: `<strong>The Perceptron uses the thresholded binary output.</strong> This is the defining characteristic. 
    The threshold (hard step function) is applied to net first, giving y ∈ {0,1}. Only then is the error (t−y) computed. 
    This means the error signal can only be −1, 0, or +1 — never a fractional value. 
    Option A describes the Delta rule. Options C and D are distractors.`
  },
  {
    q: "The Delta rule is described as 'gradient descent on the sum-squared error.' What error function E is being minimized?",
    opts: [
      "E = Σ |t − threshold(net)|",
      "E = Σ (t − sigmoid(net))²",
      "E = ½ Σ (t − net)²",
      "E = −Σ t·log(y) + (1−t)·log(1−y)"
    ],
    ans: 2,
    exp: `<strong>E = ½ Σ (t − net)²</strong> — taking ∂E/∂wᵢ = −(t−net)·xᵢ, and descending the negative gradient gives 
    Δwᵢ = η(t−net)xᵢ, which is exactly the Delta rule update. The ½ is a convenience constant that cancels the 2 from the squared term. 
    Option A is what the Perceptron minimizes (heuristically). Option B would require a sigmoid derivative term (backprop). 
    Option D is cross-entropy loss (logistic regression).`
  },
  {
    q: "Which statement about the Perceptron Convergence Theorem is TRUE?",
    opts: [
      "The Perceptron always finds the minimum mean squared error solution",
      "The Perceptron converges in finite steps if and only if data is linearly separable",
      "The Perceptron converges to the same solution as the Delta rule on linearly separable data",
      "The Perceptron converges even for non-linearly separable data given sufficient iterations"
    ],
    ans: 1,
    exp: `<strong>The theorem guarantees convergence in finite steps IF AND ONLY IF the data is linearly separable.</strong> 
    For non-separable data, it oscillates forever. Option A is false — the Delta rule minimizes MSE, not the Perceptron. 
    Option C is false — they may find different separating hyperplanes. Option D is the classic misconception; 
    on non-separable data, the Perceptron never stabilizes.`
  },
  {
    q: "Consider: two points (1,1)→class 1 and (−1,−1)→class 0. The Perceptron updates weights. After one step with η=0.5, current weights w=(0,0,0) [w₀,w₁,w₂], first sample (1,1)→target 1. What is Δw₁?",
    opts: [
      "0.5",
      "−0.5",
      "0",
      "1.0"
    ],
    ans: 0,
    exp: `<strong>Δw₁ = 0.5</strong>. net = 0·1 + 0·1 + 0·1 = 0. Threshold(0) = 0 (assuming threshold θ=0, output=0 if net≤0). 
    t=1, y=0, so error = t−y = 1. Δw₁ = η·(t−y)·x₁ = 0.5 · 1 · 1 = 0.5. 
    Similarly Δw₀=0.5 (bias, x₀=1) and Δw₂=0.5.`
  },
  {
    q: "What is the MAIN practical advantage of the Delta rule over the Perceptron rule?",
    opts: [
      "It runs faster because it skips the activation function",
      "It converges even when data is NOT linearly separable (finds best linear approximation)",
      "It produces binary outputs directly usable for classification",
      "It uses less memory because it does not store the error term"
    ],
    ans: 1,
    exp: `<strong>The Delta rule always converges — to the minimum MSE solution even for non-separable data.</strong>
    The Perceptron will oscillate endlessly on non-separable data. The Delta rule gracefully degrades to the 
    best linear approximation. Option A is backwards (Perceptron skips nothing — it just applies a threshold). 
    Option C: Delta rule outputs are continuous, not binary. Option D is a fabricated advantage.`
  },
  {
    q: "As the steepness parameter k of σ(x) = 1/(1+e^{−kx}) approaches infinity, what does the sigmoid function approach?",
    opts: [
      "The identity function f(x) = x",
      "The ReLU function max(0,x)",
      "The hard threshold (step) function used in the Perceptron",
      "A Gaussian bell curve centered at 0"
    ],
    ans: 2,
    exp: `<strong>As k→∞, sigmoid → hard threshold function.</strong> For any x>0, e^{−kx}→0 so σ→1. 
    For x<0, e^{−kx}→∞ so σ→0. At x=0, σ=0.5 always. This is the theoretical connection between 
    the Delta rule (with sigmoid activation in backprop) and the Perceptron (with hard threshold). 
    The Perceptron is the infinite-steepness limit of the sigmoid neuron.`
  },
  {
    q: "In a 2D input space (x₁, x₂), a neuron's decision boundary is the line 2 − 3x₁ + x₂ = 0. What is the slope (dx₂/dx₁) of this boundary?",
    opts: [
      "−1/3",
      "3",
      "−3",
      "2"
    ],
    ans: 1,
    exp: `<strong>Slope = 3.</strong> Rearranging: x₂ = 3x₁ − 2. 
    So dx₂/dx₁ = 3. The general form is w₀ + w₁x₁ + w₂x₂ = 0 → x₂ = −(w₁/w₂)x₁ − (w₀/w₂). 
    Here w₁=−3, w₂=1, so slope = −(−3)/1 = 3. Many students reflexively say −3 (the coefficient of x₁) — that's the trap.`
  },
  {
    q: "The Delta rule update is Δwᵢ = η(t − net)xᵢ. If t=0.8, net=0.6, η=0.1, x₂=2, what is Δw₂?",
    opts: [
      "0.02",
      "0.04",
      "0.16",
      "−0.04"
    ],
    ans: 1,
    exp: `<strong>Δw₂ = 0.04.</strong> Error = t − net = 0.8 − 0.6 = 0.2. 
    Δw₂ = η · error · x₂ = 0.1 · 0.2 · 2 = 0.04. 
    Option A (0.02) is the error if x₂=1. Option C (0.16) = error · x₂ without η. 
    Option D is negative, which would mean the weight decreases when we should be increasing it.`
  },
  {
    q: "Which of these is a key reason why the Delta rule (and NOT the Perceptron rule) generalizes to backpropagation in multilayer networks?",
    opts: [
      "The Delta rule uses binary outputs which can be stacked in layers",
      "The Delta rule's update is a true gradient — it can be composed via the chain rule through differentiable activations",
      "The Perceptron rule requires global information about the whole network",
      "The Delta rule converges faster on all tasks than the Perceptron rule"
    ],
    ans: 1,
    exp: `<strong>The Delta rule is gradient descent, so the chain rule extends it through multiple layers.</strong>
    Backpropagation is simply the Delta rule applied layer by layer using the chain rule. The key requirement 
    is that activations are differentiable (sigmoid, tanh, ReLU). The Perceptron's hard threshold has a derivative 
    of 0 almost everywhere — you can't backpropagate through it. Option A is backwards (binary outputs are the problem, not the solution). 
    Option C is false. Option D is not generally true.`
  },
  {
    q: "Both Perceptron and Delta rules use Δwᵢ = η(t−y)xᵢ, but produce different decision boundaries on the same linearly separable dataset. Why?",
    opts: [
      "They use different learning rates",
      "The Perceptron minimizes classification error; Delta minimizes MSE — different objectives lead to different boundaries",
      "The Delta rule adds a regularization term the Perceptron lacks",
      "They are mathematically identical on linearly separable data"
    ],
    ans: 1,
    exp: `<strong>Different objectives → different solutions, even on separable data.</strong>
    The Perceptron finds ANY hyperplane that separates the classes (stops as soon as all points are correct). 
    The Delta rule minimizes the sum-squared error in the continuous output space, which pushes the boundary 
    further from misclassified points even after they're technically "correct." They will generally converge 
    to different weight vectors. Option D is a common misconception — same formula, very different dynamics.`
  }
];

let answered = new Array(questions.length).fill(false);
let correct = 0;

function buildQuiz() {
  const container = document.getElementById('quiz-container');
  container.innerHTML = '';
  questions.forEach((q, qi) => {
    const div = document.createElement('div');
    div.className = 'quiz-question';
    div.id = 'q' + qi;
    div.innerHTML = `
      <div class="q-text"><span style="color:var(--muted);font-size:11px;letter-spacing:2px;">Q${qi+1} / ${questions.length}</span><br><br>${q.q}</div>
      <div class="options">
        ${q.opts.map((o,oi) => `<button class="opt" onclick="answer(${qi},${oi})" id="opt-${qi}-${oi}">${String.fromCharCode(65+oi)}) ${o}</button>`).join('')}
      </div>
      <div class="explanation" id="exp-${qi}">${q.exp}</div>
    `;
    container.appendChild(div);
  });
  updateScore();
}

function answer(qi, oi) {
  if (answered[qi]) return;
  answered[qi] = true;
  const isCorrect = oi === questions[qi].ans;
  if (isCorrect) correct++;

  for (let i = 0; i < questions[qi].opts.length; i++) {
    const btn = document.getElementById(`opt-${qi}-${i}`);
    btn.classList.add('revealed');
    if (i === questions[qi].ans) btn.classList.add('correct');
    else if (i === oi && !isCorrect) btn.classList.add('wrong');
  }
  document.getElementById(`exp-${qi}`).classList.add('show');
  updateScore();
}

function updateScore() {
  const total = answered.filter(Boolean).length;
  document.getElementById('scoreNum').textContent = correct + '/' + total;
  document.getElementById('scoreNum').style.color = 
    total === 0 ? 'var(--accent)' : (correct/total > 0.7 ? 'var(--correct)' : 'var(--accent2)');
}

function resetQuiz() {
  answered = new Array(questions.length).fill(false);
  correct = 0;
  buildQuiz();
}

// Init
buildQuiz();
setTimeout(() => { drawLine(); drawSigmoid(); }, 100);
</script>
</body>
</html>
